




 










{}








{}

[1]{}
[1]{}

{{}}
[1]{
{

{Lemma}
[lemma]{Theorem}

[1]{ #1}









{fandm}


{Department of Computer Science, Franklin   Marshall College, Lancaster, Pennsylvania, USA}

{erik.talvitie@fandm.edu}



]



  


 

In the reinforcement learning problem, an agent interacts with an
environment, receiving rewards along the way that indicate the quality
of its decisions. The agent's task is to learn to behave in a way that
maximizes reward. Model-based reinforcement learning (MBRL) techniques
approach this problem by learning a predictive model of the
environment and applying a planning algorithm to the model to make
decisions. Intuitively and theoretically , there
are many advantages to learning a model of the environment, but MBRL is
challenging in practice, since even seemingly minor flaws in the model
or the planner can result in catastrophic failure. As a result,
model-based methods have generally not been successful in large-scale
problems, with only a few notable exceptions
.

This paper addresses an important but understudied problem in MBRL:
learning a reward function. It is common for work in model learning to
ignore the reward function   oh2015action, chiappa2017recurrent} or, if the model will be used
for planning, to assume the reward function is given
  ebert2017self}. Indeed, it is true that if an accurate model of the
environment's dynamics can be learned, reward learning is relatively
straightforward -- the two problems can be productively
decoupled. However, in this paper we will see that when the model
class is { (i.e. that the representation does not
admit a perfectly accurate model), as is inevitable in problems of
genuine interest, the two learning problems are inherently entangled.



To better understand how the limitations of the dynamics model impact
reward learning, consider Shooter, a simplified video game example
introduced by , pictured in Figure . At
the bottom of the screen is a spaceship which can move left and right
and fire bullets, which fly upward. When the ship fires a bullet the
agent receives -1 reward. Near the top of the screen are three
targets. When a bullet hits a target in the middle (bullseye), the
target explodes and the agent receives 20 reward; otherwise a hit is
worth 10 reward. Figure  shows the explosions that
indicate how much reward the agent receives.



It is typical to decompose the model learning problem into two
objectives: dynamics learning and reward learning. In the former the
agent must learn to map an input state and action to the next
state. In the latter the agent must learn to map a state and action to
reward. In this example the agent might learn to associate the
presence of explosions with reward. However, this decomposed approach
can fail when the dynamics model is imperfect.

For instance, say the dynamics model in this case is a factored MDP,
which predicts the value of each pixel in the next image based on the
 neighborhood centered on the pixel. Figure
b shows a short { from such
a model, sampling each state based on the previous sampled state. The
second image in the rollout illustrates the model's flaw: when
predicting the pixel marked with a question mark the model cannot
account for the presence of the bullet under the target. Hence, errors
appear in the subsequent image (marked with red outlines).

What reward should be associated with this erroneous image? The value
the learned model assigns will have a dramatic impact on the extent to
which the model is useful for planning and yet it is clear that no
amount of traditional data associating environment states with rewards
can answer this question. Even a provided, ``perfect'' reward function
would not answer this question; a reward function could assign any
value to this state and still be perfectly accurate in states that are
reachable in the environment. Intuitively it seems that the best case
for planning would be to predict 20 reward for the flawed state,
preserving the semantics that a target has been hit in the
bullseye. Note, however that this interpretation of the image is
specific to this particular flawed model; the reward model's quality
depends on its behavior in states generated by the { rather than
the environment.

The remainder of this paper formalizes this intuition. Section
 presents a novel error bound on value functions in
terms of reward error, taking into account the rewards in flawed states
generated by the model. In Section  the practical
implications of this theoretical insight are discussed, leading to an
extension of the existing Hallucinated DAgger-MC algorithm, which
provides theoretical guarantees in deterministic MDPs, even when the
model class is misspecified. Section 
demonstrates empirically that the approach suggested by the
theoretical results can produce good planning performance with a
flawed model, while reward models learned in the typical manner (or
even ``perfect'' reward functions) can lead to catastrophic planning
failure.





We focus on { (MDP). The environment's
initial state  is drawn from a distribution
. At each step  the environment is in a state . The agent
selects an action  which causes the environment
to transition to a new state sampled from the transition distribution:
.  The environment also emits a reward,
. We assume that rewards are bounded within
.

A {  specifies a way to behave in the MDP. Let
 be the probability that  chooses action  in
state . For a sequence of actions  let
 be the probability of
reaching  by starting in  and taking the actions in the
sequence. For any state , action , and policy , let
 be the state-action distribution obtained after 
steps, starting with state  and action  and thereafter following
policy . For a state action distribution , let
.  We let
 be the set of states reachable in finite time by some
policy with non-zero probability. One may only observe the behavior
of  and  in states contained in .

The -step { of a policy, 
represents the expected discounted sum of rewards obtained by taking
action  in state  and executing  for an additional 
steps:
$Q^^{T}   D^t_{s, a, } R_{s'}^{a'}$.
Let the -step {
. Let
, and . The agent's
goal will be to learn a policy  that maximizes
.

In MBRL we seek to learn a dynamics model , approximating
, and a reward model , approximating , and then to use
the combined model  to produce a policy via a
planning algorithm. We let , , and 
represent the corresponding quantities using the learned model. We
assume that  and  are defined over
; there may be states in
 for which  and  are effectively undefined,
and it may not be known { which states these are.

Let  represent the {, the set of
models the learning algorithm could possibly produce and
correspondingly let  be the {. In
this work we are most interested in the common case that the dynamics
model is {: there is no 
that matches  in every . In this case
it is impossible to learn a perfectly accurate model; the agent must
make good decisions despite flaws in the learned model. The results in
this paper also permit the reward model to be similarly misspecified.



For ease of analysis we focus our attention on the simple one-ply
Monte Carlo planning algorithm (one-ply MC), similar to the ``rollout
algorithm'' . For every state-action pair
, the planner executes  -step sample rollouts using
, starting at , taking action , and then following a
{ . At each step of the rollout, 
gives the reward. Let  be the average discounted return
of the rollouts starting with state  and action . For large ,
 will closely approximate 
. The execution policy  will be
greedy with respect to . We can place bounds on
the quality of .

For a policy  and state-action distribution , let
 be the error in the -step
state-action values the model assigns to the policy:
$^{ =   (s, a) - _T^{(s, a)|state distribution  and policy  let $D_{}(s, a) =
^{  D_{^{t+1}(s, a)$.
The following is straightforwardly adapted from an
existing bound .

  Let  be the value
  function returned by applying depth  one-ply Monte Carlo to the
  model  with rollout policy . Let  be
  greedy w.r.t. . For any policy  and state-distribution
  ,
  

Inequality  is Lemma 
in the deterministic case, the bound in terms
of the one-step prediction error of . Inequality
 gives the bound in terms of the error in the discounted
distribution of states along -step rollouts. Though this is the
tightest bound of the three, in practice it is difficult to optimize
this objective directly. Inequality  gives the
bound in terms of {, so called because
it considers the accuracy of the model's predictions based on states
generated from its own sample rollouts (), rather than states
generated by the environment (). 

To optimize hallucinated error, the model can be rolled out in
parallel with the environment, and trained to predict the next
environment state from each ``hallucinated'' state in the model
rollout.  shows that this approach can
dramatically improve planning performance when the model class is
mispecified. Similar approaches have also had empirical success in
MBRL tasks  and
sequence prediction tasks   oh2015action,bengio2015scheduled}.

 shows that the relative tightness of the
hallucinated error bound does not hold for general stochastic dynamics
or for arbitrary rollout policies. However, note that these
assumptions are not as limiting as they first appear. By far the most
common rollout policy chooses actions uniformly randomly, and is thus
blind. Furthermore, though  is assumed to be deterministic, it is
also assumed to be too complex to be practically captured by
. From the agent's perspective, un-modeled complexity will
manifest as apparent stochasticity. For example
 learned dynamics models of Atari 2600 games,
which are fully deterministic ;
human players often perceive them to be stochastic due to their
complexity. For the remainder of the paper we focus on the special
case of deterministic dynamics and blind rollout policies.



As suggested by , there is a straightforward
extension of Theorem  to account for reward error.



The next section discusses the practical and conceptual implications of this result
for MBRL algorithms and extends an existing MBRL algorithm to
incorporate this insight.



This is not the first observation that the reward model should be
specialized to the dynamics model .  argued as
we have that when the model or planner are limited in some way, reward
functions other than the true reward may lead to better planning
performance. Accordingly, policy gradient approaches have been
employed to learn reward functions for use with online planning
algorithms, providing a benefit even when the reward function is known
. 
take this idea to its logical extreme, treating the entire model and
even the planning algorithm itself as a policy parameterization,
adapting them to directly improve control performance rather than to
minimize any measure of prediction error. Though appealing in its
directness, this approach offers little theoretical insight into what
makes a model useful for planning. Furthermore, there are advantages
to optimizing quantities other than control performance; this allows
the model to exploit incoming data even when it is unclear how to
improve the agent's policy (for instance if the agent has seen little
reward). Theorem  provides more specific guidance about
how to choose amongst a set of flawed models. Rather than attempting
to directly optimize control performance, this result suggests that we
can take advantage of reward error signals while still offering
guarantees in terms of control performance.

It is notable that, unlike Theorem , Theorem
 does not contain a term measuring dynamics
error. Certainly the dynamics model is implicitly important; for some
choices of  the hallucinated reward error can be made very
small while for others it may be irreducibly high (for instance if
 simply loops on a single state). Nevertheless, low
hallucinated reward error does not require that the dynamics model
place high probability on ``correct'' states. In fact, it may be that
dynamics entirely unrelated to the environment yield the best reward
predictions. This intriguingly suggests that the dynamics
model and reward model parameters could be adapted together to
optimize hallucinated reward error. Arguably, the recently introduced Predictrons
 and Value Prediction Networks
 are attempts to do just this -- they adapt the
model's dynamics solely to improve reward prediction. We can see
Theorem  as theoretical support for these approaches and
encouragement of more study in this direction. Still, in practice it
may be much harder to learn to predict reward sequences than state
sequences, especially when the reward signal is sparse. Also, the
relationship between reward prediction error and dynamics model parameters
can be highly complex, which may make theoretical performance
guarantees difficult.

Another possible interpretation of Theorem  is that the
reward model should be customized to the dynamics model. That is, if
we hold the dynamics model fixed, then the result gives a clear
objective for the reward model. Theorem  suggests an
algorithmic structure where the dynamics model is trained via its own
objective, and the reward model is then trained to minimize
hallucinated error with respect to the learned dynamics model. The
clear downside of this approach is that it will not in general find
the best combination of dynamics model and reward model; it could be
that a less accurate dynamics model results in lower hallucinated
reward error. The advantage is that it allows us to effectively exploit the
prediction error signal for the dynamics model and removes the
circular dependence between the dynamics model and the reward model.

In this paper we explore this avenue by extending the existing
Hallucinated DAgger-MC algorithm . Because the
resulting algorithm is very similar to the original, we leave a
detailed description and analysis to the appendix and here focus on
key, high-level points. Section  presents
empirical results illustrating the practical impact of training the reward model
to minimize hallucinated error.



The ``Data Aggregator'' (DAgger) algorithm 
was the first practically implementable MBRL algorithm with
performance guarantees agnostic to the model class. It did, however,
require that the planner be near optimal. DAgger-MC
 relaxed this assumption, accounting for
the limitations of a particular suboptimal planner (one-ply
MC). Hallucinated DAgger-MC (or H-DAgger-MC) 
altered DAgger-MC to optimize the hallucinated error, rather than the
one-step error. All of these algorithms were presented under the
assumption that the reward function was known {. As we
will see in Section , the reward function cannot
be ignored; even when the reward function is given, these algorithms
can fail catastrophically due to the interaction between the reward
function and small errors in the dynamics model.

At a high level, H-DAgger-MC proceeds in iterations. In each iteration
a batch of data is gathered by sampling state-action pairs using a
mixture of the current plan and an ``exploration distribution'' (to
ensure that important states are visited, even if the plan would not
visit them). The rollout policy is used to generate parallel rollouts
in the environment and model from these sampled state-action pairs,
which form the training examples (with model state as input and
environment state as output). The collected data is used to update the
dynamics model, which is then used to produce a new plan to be used in
the next iteration. We simply augment H-DAgger-MC, adding a reward
learning step to each iteration (rather than assuming the reward is
given). In each rollout, training examples mapping ``hallucinated''
model states to the real environment rewards are collected and used to
update the reward model.

The extended H-DAgger-MC algorithm offers theoretical guarantees
similar to those of the original algorithm. Essentially, if

  distribution of a good policy, 
  are both no-regret, and
  reward error model with respect to the lowest hallucinated
  prediction error model in ,

then in the limit H-DAgger-MC will produce a good policy. As discussed
in Section , this does { guarantee that
H-DAgger-MC will find the best performing combination of dynamics
model and reward model, since the training of the dynamics model does
not take hallucinated reward error into account. It is, however, an
improvement over the original H-DAgger-MC result in that good
performance can be assured even if there is no low error
dynamics model in , as long as there is a low error
reward model in .

For completeness' sake, a more detailed description and analysis of the
algorithm can be found in the appendix. Here we turn to an empirical
evaluation of the algorithm.



In this section we illustrate the impact of optimizing hallucinated
reward error in the Shooter example described in Section
 using both DAgger-MC and H-DAgger-MC  code for these experiments is available at .}. The
one-ply MC planner used 50 uniformly random rollouts of depth 20 per
action at every step. The exploration distribution was generated by
following the optimal policy with  probability of
termination at each step. The discount factor was . In
each iteration 500 training rollouts were generated and the resulting
policy was evaluated in an episode of length 30. The discounted return
obtained by the policy in each iteration is reported, averaged over 50
trials.

The dynamics model for each pixel was learned using Context Tree
Switching , similar to the FAC-CTW algorithm
. At each position the model takes as input
the values of the pixels in a  neighborhood around the
position in the previous timestep. Data was shared across all
positions. The reward was approximated with a linear function for each
action, learned via stochastic weighted gradient descent. The feature
representation contained a binary feature for each possible
 configuration of pixels at each position. This
representation admits a perfectly accurate reward model. The
qualitative observations presented in this section were robust to a
wide range of choices of step size for gradient descent. Here, in each
experiment the best performing step size for each approach is selected
from 0.005, 0.01, 0.05, 0.1, and 0.5.

In the experiments a practical alteration has been made to the
H-DAgger-MC algorithm. H-DAgger-MC requires an ``unrolled''
dynamics model (with a separate model for each step of the rollout,
each making predictions based on the output of the previous
model). While this is important for H-DAgger-MC's theoretical
guarantees,  found empirically that a single
dynamics model for all steps could be learned, provided that the training rollouts
had limited depth. Following , in the first 10
iterations only the first example from each training rollout is added
to the dynamics model dataset; thereafter only the first two examples
are added. The entire rollout was used to train the reward
model. DAgger-MC does not require an unrolled dynamics model or
truncated training rollouts and was
implemented as originally presented, with
a single dynamics model and full training rollouts . 



We consider both DAgger-MC and H-DAgger-MC with a perfect reward
model, a reward model trained only on environment states during
rollouts, and a reward model trained on ``hallucinated'' states as described in
Section . The perfect reward model is one that
someone familiar with the rules of the game would likely specify; it
simply checks for the presence of explosions in the three target
positions and gives the appropriate value if an explosion is present
or 0 otherwise (subtracting 1 if the action is ``shoot''). Results are
presented in three variations on the Shooter problem.


    }  }  }      variations on the Shooter domain.}
  




In the first experiment we apply these algorithms to Shooter, as
described in Section . Here, the dynamics model uses a
 neighborhood, which is sufficient to make perfectly
accurate predictions for every pixel. Figure a shows the
discounted return of the policies generated by DAgger-MC and
H-DAgger-MC, averaged over 50 independent trials. The shaded region
surrounding each curve represents a 95uniform random policy (with a 95one-ply MC planner using a perfect model.

Unsurprisingly, the performance DAgger-MC is comparable to that of
planning with the perfect model. As observed by
, with the perfect reward model H-DAgger-MC
performs slightly worse than DAgger-MC; the dynamics model in
H-DAgger-MC receives noisier data and is thus less
accurate. Interestingly, we can now see that the learned reward model
yields better performance than the perfect reward model, even without
hallucinated training! The perfect reward model relies on specific
screen configurations that are less likely to appear in flawed sample
rollouts, but the learned reward model generalizes to screens not seen
during training. Of course, it is coincidental that this
generalization is beneficial; under standard training the reward model
is only trained in environment states, giving no guidance in erroneous
model states. Hallucinated training specifically trains the reward
model to make reasonable predictions during model rollouts, so it
yields better performance, comparable with that of DAgger-MC. Thus we
see that learning the reward function in this way mitigates a
shortcoming of H-DAgger-MC, making it more effective in practice when
a perfectly accurate model can be learned.



Next we consider a version of shooter presented by
 in which the bullseye in each target moves
from side to side, making the environment second-order Markov. Because
the model is Markov, it cannot accurately predict the movement of the
bullseyes, though the representation is sufficient to accurately
predict every other pixel.

Figure b shows the results. As
 observed, DAgger-MC fails catastrophically in
this case. Though the model's limitation only prevents it from
accurately predicting the bullseyes, the resulting errors compound
during rollouts, quickly rendering them useless. As previously
observed, H-DAgger-MC performs much better, as it trains the model to
produce more stable rollouts. In both cases we see again that the
learned reward models outperform the perfect reward model, and
hallucinated reward training yields the best performance, even
allowing DAgger-MC to perform better than the random policy.



We can see the importance of hallucinated reward training even more
clearly when we consider the original Shooter domain (with static
bullseyes), but limit the size of the neighborhood used to predict each
pixel, as described in Section . Figure
c shows the results. Once again DAgger-MC
fails. Again we see that the learned reward models yield better
performance than the perfect reward function, and that hallucinated
training guides the reward model to be useful for planning, despite
the flaws in the dynamics model.

In this case, we can see that H-DAgger-MC { fails when
combined with the perfect reward model, and performs poorly with the
reward model trained only on environment states. Hallucinated training
helps the dynamics model produce stable sample rollouts, but does not
correct the fundamental limitation: the dynamics model cannot
accurately predict the shape of the explosion when a target is hit. As
a result, a reward model that bases its predictions only the
explosions that occur in the environment will consistently fail to
predict reward when the agent hits a target in sample
rollouts. Hallucinated training, in contrast, specializes the reward
model to the flawed dynamics model, allowing for performance
comparable to planning with a perfect model.



This paper has introduced hallucinated reward error, which measures
the extent to which the rewards in a sample rollout from the model
match the rewards in a parallel rollout from the environment. Under
some conditions, this quantity is more tightly related to control
performance than the more traditional measure of model quality (reward
error in environment states plus error in state
transition). Empirically we have seen that when the dynamics model is
flawed, reward functions learned in the typical manner and even 
``perfect'' reward functions given { can lead to
catastrophic planning failure. When the reward function
is trained to minimize hallucinated reward error, it specifically
accounts for the model's flaws, significantly improving performance.



This work was supported by NSF grant IIS-1552533. Thanks also to
Michael Bowling for his valuable input and to Joel Veness for his
freely available FAC-CTW and CTS implementations (//jveness.info/software/}).







Hallucinated DAgger-MC, like earlier variations on DAgger, requires
the ability to reset to the initial state distribution  and also
the ability to reset to an ``exploration distribution'' . The
exploration distribution ideally ensures that the agent will encounter
states that would be visited by a good policy. The performance bound
for H-DAgger-MC depends in part on the quality of the selected .

In addition to assuming a particular form for the planner (one-ply MC
with a blind rollout policy), H-DAgger-MC requires the dynamics model
to be ``unrolled''. Rather than learning a single ,
H-DAgger-MC learns a set
, where
model  is responsible for predicting the outcome of step
 of a rollout, given the state sampled from . While
this impractical condition is important theoretically,
 showed that in practice a single
 can be used for all steps; the experiments in Section
 make use of this practical alteration.

Algorithm  augments H-DAgger-MC to learn a reward
model as well as a dynamics model. In particular, H-DAgger-MC proceeds
in iterations, each iteration producing a new plan, which is turn used
to collect data to train a new model. In each iteration state-action
pairs are sampled using the current plan and the exploration
distribution (lines 7-13), and then the world and model are rolled out
in parallel to generate hallucinated training examples (lines
14-21). The resulting data is used to update the model. We simply add
a reward model learning process, and collect training examples along
with the state transition examples during the rollout. After both parts of
the model have been updated, a new plan is generated for the
subsequent iteration. Note that while the dynamics model is
``unrolled'', there is only a single reward model that is responsible
for predicting the reward at every step of the rollout. We assume that
the reward learning algorithm is performing a weighted regression
(where each training example is weighted by  for the
rollout step  in which it occurred).





We now derive theoretical guarantees for this new version of
H-DAgger-MC. The analysis is similar to that of existing DAgger
variants   talvitie2017self}, but the proof is included for completeness. Let
 be the distribution from which H-DAgger-MC samples a training
example at depth  (lines 7-13 to pick an initial state-action pair,
lines 14-21 to roll out). Define the average error of the dynamics
model at depth  to be

^{t}_{prd} = {N}^N   a)  [1 -_{n}^{t}(^{a} 
Let $_n}(s, z, a) = |R(s, a) - _n(z,
a)|$ and let

_{hrwd} = {N}^N ^T
  [_n}(s,
  z, a)|]

be the average reward model error. Finally, let  be the
distribution from which H-DAgger-MC samples  and  during the
rollout in lines 14-21. The error of the reward model with respect to
these environment states is

_{erwd} = {N}^N ^{T}
  [|R(s, a) -
(s, a)|].


For a policy , let

represent the mismatch between the discounted state-action
distribution under  and the exploration distribution . Now,
consider the sequence of policies  generated by
H-DAgger-MC. Let  be the uniform mixture over all policies
in the sequence. Let
$_{mc} = {N} {1 -  ^N
_n - ^{_{T,n} + {1 - 
_T - V^{_T $
be the error induced by the choice of planning algorithm, averaged
over all iterations.
 
  In H-DAgger-MC, the policies  are such that for any
  policy ,

   &(s) - V^{}(s)  {1-c^_{hrwd} + _{mc}&{1- c^_{erwd} + 2M
  ^{T-1} ^t_{prd}_{mc}.




Note that this result holds for { comparison policy
. Thus, if  is small and the learned models
have low error, then if  is similar to the state-action
distribution under { good policy,  will compare
favorably to it. That said, Lemma  shares the
limitations of the comparable results for the other DAgger
algorithms. It focuses on the L1 loss, which is not always a practical
learning objective. It also assumes that the expected loss at each
iteration can be computed exactly (i.e. that there are infinitely many
samples per iteration). It also applies to the average policy
, rather than . 
discuss extensions that address more practical loss functions, finite
sample bounds, and results for .

Lemma  effectively says that { the models
have low training error, the resulting policy will be good. It does
not promise that the models will have low training error. Following
 note that  and
 can each be interpreted as the average loss of
an online learner on the problem defined by the aggregated
datasets. Then for each horizon depth  let
 be the error of the best dynamics model in
 under the training distribution at that depth, in
retrospect. Specifically,

  ^{t}_{} =   }{N}^N   H^t_n} [1 - P'(^{a} 
Similarly, let

_{} = }{N}  1}^N ^T    ^t_n} [(s, z, a)]

 be the error of the best reward
model in  in retrospect.

The average regret for the dynamics model at depth  is
$^t_{prgt} = _{prd}^t -
_{}^t$.
For the reward model it is
$_{rrgt} = _{hrwd} -
_{}$.
For a no-regret online learning algorithm, average regret approaches 0
as . This gives the following bound on
H-DAgger-MC's performance in terms of model regret.


Theorem  says that if  contains a
low-error reward model relative to the learned dynamics models then,
as discussed above, if  is small and  visits
important states, the resulting policy will yield good performance.
If  and  contain perfect models, 
will be comparable to the plan generated by the perfect model.

As noted by , this result does {
promise that H-DAgger-MC will eventually achieve the performance of
the best available set of dynamics models. The model at each rollout
depth is trained to minimize prediction error given the input
distribution provided by the shallower models without regard for the
effect on deeper models. It is possible that better overall error
could be achieved by { the prediction error at one
depth in exchange for a favorable state distribution for deeper
models. Similarly, as discussed in Section ,
H-DAgger-MC will not necessarily achieve the performance of the best
available combination of dynamics and reward models. The dynamics
model is trained without regard for the impact on the reward model. It
could be that a dynamics model with higher prediction error would
allow for lower hallucinated reward error. H-DAgger-MC does not take
this possibility into account.

