





 

























{Theorem}
[theorem]{Proposition}
[theorem]{Corollary}
[theorem]{Definition}
[theorem]{Lemma}
[theorem]{Remark}
[theorem]{Example}


}



[1]{}
[1]{
{
{
{v_{i,j}}
[2]{v_{#1,#2}}


{}
{}

{}
{}


{{}
{{}

{}
{}

{}
{}

{{}
{{}

{{}




































Technology} 
 







De Bruijn sequence 









De Bruijn sequences were rediscovered many times over the years, starting from 1894 by Flye-Sainte Marie~, and finally by De Bruijn himself in 1946~. For two positive non-zero integers,  and , an -De Bruijn (-DB, for abbreviation) sequence is a cyclic sequence over the alphabet  in which every word of length  over  appears exactly once as a subword. 
It is cyclic in the sense that some words are generated by  concatenating the suffix of length  of the sequence, with its prefix of length .

A construction for a family of -DB sequences is an algorithm that receives the two arguments,  and  (occasionally,  is fixed and only  is given as argument), and outputs an -DB sequence. Obviously, a trivial time lower bound for a construction is , as this is the exact length of an -DB sequence. Many constructions for a variety of families of De Bruijn sequences are known, (for example, ) and some of them are also time optimal.

A specifically famous family of -DB sequences is the prefer-max family~, which is constructed by the well-known ``granddaddy" greedy algorithm~ (see also~). The algorithm constructs the sequence symbol by symbol, where at each step the maximal value is added to the initial segment constructed so far, so that the new suffix of length  does not appear elsewhere. A symmetric approach produces the prefer-min DB sequence. Besides this highly inefficient algorithm,
many other constructions for the prefer-max and prefer-min sequences have been proposed in the literature. A classic result by Fredricksen and Kessler~, and Fredricksen and Maiorana~ shows that the prefer-max sequence is in fact a concatenation of certain words, a result we use in this work. This block construction was later proved to be time optimal in~. Another efficient block concatenation construction was suggested in~.

A common and important way of generating DB sequences is by using a shift-rule (also named a shift-register). A shift-rule for an -DB sequence receives a word of length , , as an input, and outputs the symbol that follows  at the sequence. Here,  and  are parameters of the algorithm. Obviously, a shift-rule must run in  time since it must read every symbol in its input to produce the correct output. Shift-rules are important since, unlike block constructions,  they can be applied on words that appear at the middle of the sequence. 


Several efficient shift-rules for DB sequences are known for  (see~ for a comprehensive list). However, only recently efficient shift-rules were discovered for non-binary sequences. Sawada et al.~ introduced a new family of DB sequences and provided a linear time shift-rule for these sequences. Amram et al.~ introduced an efficient shift-rule for the famous prefer-max and prefer-min DB sequences.


We note that, generally, a construction for a DB sequence provides an exponential time shift-rule, since, on many inputs,  it is required to construct almost the whole sequence to find the desired symbol the shift-rule should output. On the other hand, a shift-rule for a DB sequence provides a construction in  time, by finding the next symbols one by one, which is not an optimal approach. 

We see that none of these two methods, a general construction and a shift-rule, dominates the other, and we propose here a third way, which generalizes both methods. A  ( for abbreviation) for an -DB sequence is an algorithm that receives two arguments: a word, , of length , and a positive integer . The  outputs the  symbols that follow  at the sequence. Since the algorithm must read its input and must write  symbols,  is a trivial time lower bound for a . An optimal  provides an optimal shift-rule when used with . In addition, an optimal  provides an optimal construction by invoking it with , or by invoking it  times with  (for example).

Although a  is defined here for the first time, researchers have noted the advantages behind this notion, and mentioned that their shift-rule possesses the properties we seek for in this paper. In~ Sawada et al. described a shift-rule with -amortized time per bit. As this seems to contradict the trivial time lower bound mentioned earlier, this statement requires a clarification. The shift-rule proposed in~ has the interesting property that after using it once, it can be invoked  more times and, by carefully retaining data from one invocation to another, it can produce the next  symbols in -amortized time. Hence, in fact, Sawada et al. noted and mentioned that their shift-rule also forms a time optimal . A similar remark can be found in~.

In this paper, we present an optimal  for the well-known prefer-max and prefer-min DB sequences.
Our  construction takes advantage of the seminal block-construction of~ for those sequences, in the following manner.
The sequences are constructed in~ as a concatenation of certain words: . In Section~ we note that a  can be constructed by solving a similar problem, named . The problem is to find a word  that completes  into a suffix of  for some . If this suffix is of insufficient length, we use another algorithm, presented in Section~, which finds the words that follow  in that block-construction. In Section~ we present a filling-the-gap algorithm which provides, as explained, a  for the prefer-min and prefer-max sequences. In addition, in Section~ we define notations used throughout the paper, and conclusions are given in Section~.




For an integer , consider the alphabet , ordered naturally by . Hence, the set of all words over , denoted , is totally ordered by the lexicographic order, which we simply denote by `'. As usual, the empty word is denoted by . We say that a word, , is an  if . Furthermore, the  of  is the prefix of  of length , and the  of  is the suffix of  of length . These notions are defined, of course, only when . 

 A word, , is a  of a word, , if  and . In addition,  is a  of , if  and . Note that a word  can be equal to some of its non-trivial rotations. This happens when  for some non-empty word, , and an integer . In this case,  is said to be . A ~ is a non-empty word that is strictly smaller than all its non-trivial rotations. Hence, in particular, a Lyndon word is aperiodic. 
 


The prefer-max -DB sequence is the cyclic sequence  constructed by the greedy algorithm that starts with , and repeatedly adds the largest possible symbol in  so that no -word appears twice as a subword of this sequence, until the sequence length is , and then rotates the obtained sequence to the left  times. As an example, for  and , this greedy process produces the sequence:  and the prefer-max -DB sequence is: . Analogously, the prefer-min -DB sequence is produced by the greedy algorithm which starts with , and repeatedly concatenates the smallest possible symbol so that no repetition occurs, and afterwards rotates the resulting sequence to the left  times. 

We note that the prefer-min -DB sequence and the prefer-max -DB sequence can be derived one from the other by replacing each symbol, , with .  Therefore, a  for one of these sequences can be easily transformed into a  for the other one as well. We present here a  algorithm for the prefer-min -DB sequence. 

From this point on, we refer to  and  as fixed, unknown, parameters, larger than  (to avoid trivialities). We measure time complexity of all algorithms given here in terms of the parameter , assuming that arithmetic operations can be computed in constant time, regardless of how large the numbers they are applied on. 

Let   be the (finite) sequence of all Lyndon words over  of length at most , sorted lexicographically. Let  be the number of all Lyndon words over  whose length divides , and let  be an enumeration of them, sorted lexicographically. For a Lyndon word, , let . Since  divides ,  is a positive integer. Note that every  is equal to some  such that . The main result of~ (with a straightforward adaptation) is:



 As an example, for  we concatenate in an increasing order all Lyndon words of length one or three. We get the following sequence, decomposed into Lyndon words:
 (3,3)$

As said, our strategy in constructing a  for the prefer-min sequence is to fill the gap between the input, , to a word  in the sequence, and then to concatenate Lyndon words until we find the required  symbols that follow . To this end, we refer to the sequence  as cyclic, meaning that,
for  and , we set . 


} Algorithm  }



As a first step in constructing a  algorithm, we analyze a relatively simple case. By Theorem~, for every  the sequence  is a prefix of the prefer-min  sequence. We consider the case where we are given an -word, , that happens to be a suffix of . 
To find the next  symbols, we can compute the next words in the block construction, , so that the sequence  is of length at least .

For dealing with this restricted case, we design an algorithm that computes efficiently the function: . Moreover, for technical reasons that will arise later, we also want to apply the algorithm over Lyndon words whose length does not necessarily divide . Therefore, for a Lyndon word,
, we define  to be the lexicographically smallest  such that
. For , we define    (that is, ).
In this section, we present an  algorithm with  time complexity. 



 Algorithm~ computes  in  time.



In~, Duval describes an algorithm to build the next Lyndon word from a given one and proves:








Note that the length of the output of the algorithm may not divide . However, we use it to construct a naive algorithm which achieves that goal, with a time complexity of . We first describe this naive version, which merely invokes Algorithm~ several times, and then improve it to run in linear time.






Note that , Algorithm~ outputs . At each iteration of the loop in lines 12-14, the algorithm invokes Duval's algorithm, until it finds a Lyndon word whose length divides . This establishes a worst case runtime of . 

The reader may note that the  { instructions in lines 5-7 and lines 8-10 can be omitted. However, we aim to construct a linear time  algorithm, and we do that by modifying the { loop. Then, it will be important that the loop acts on words whose length is larger than . Thus, lines 5-7 and 8-10 are added to simplify the comparison between this naive  version and our linear time  version.

To improve the runtime of this algorithm, we identify cases in which the outcome of several loop iterations can be computed directly. These are the cases in which calling Duval's algorithm again and again results in concatenating the same sequence several times. For illustration, assume that at some point the algorithm reaches line 12 when  stores a word , such that . The Lyndon words that follow  are:
{2} .



In order to prove Algorithm~ correctness, we show that both algorithms, Algorithm~ and Algorithm~, have the same output for every legal input. We start with two observations, derived from Duval's algorithm.



, if , then .


 

, if , then .


In sketch, these observations are proved as follows: If , then   for some  and . Clearly, Corollary~ follows. Also, note that  that , which proofs Corollary~.




From these two observations we can deduce the following conclusion, discussing the similarity between the two algorithms when entering the { loop:




The following invariant holds for both Algorithm~ and Algorithm~ whenever the {} loop starts:  .




The next lemma shows that every execution of the { loop in Algorithm~ corresponds to several executions of the { loop in Algorithm~.



Let  be a Lyndon word such that . Let  be as in lines 13-16 of Algorithm~. Then, for :






Lemma~ states that each execution of the { loop of Algorithm~ corresponds to  executions of the { loop of Algorithm~. Therefore, we conclude:


, the output of Algorithm~ over the input , is equal to the output of Algorithm~ over the input .



It is left to prove that our runtime is linear. For this purpose, consider an execution of Algorithm~ on input , and assume that the execution reaches line 12 (otherwise, the algorithm terminates in line 2 after  steps, or in line 6 after  steps). We need to show that the loop terminates after  steps. This fact follows from the following observation. The loop terminates when , and after each loop iteration the value  decreases by at least half. 


To prove this fact in a precise manner, we introduce a few notations.

     loop.
    
     loop for the first time.
    
     loop .







For , we have .





Relying on this lemma, we can now analyze the runtime of our algorithm and prove Proposition~.

















The fact that   can be computed efficiently is useful for designing an efficient  algorithm. Given an -word, , assume that  is a suffix of . In this case, several invocations of our  algorithm produce the -word that follows  at the prefer-min sequence. For taking this approach, first, it is required to find a Lyndon word, , and a word, , such that  is a suffix of . This implies that a  algorithm for the prefer-min sequence can be derived from a solution to another problem we propose in this section: { ( for abbreviation).






For an -word  we write , if the following hold:

        




We leave for the reader to verify that  is well-defined, meaning that for every -word,  only a single pair, , satisfies the conditions of Definition~. We remark that it is possible that  where . This occurs in the case where  is a concatenation of a suffix of the prefer-min sequence with a prefix of it. For example, if , then  since  is a suffix of .


Note that  can be trivially computed by concatenating Lyndon words and searching for . However, this naive solution is highly inefficient as  may appear anywhere in the prefer-min sequence. Hence, for constructing an efficient  in the way described above, we need an efficient -algorithm. 





There is also another issue concerning the suggested approach, which requires attention. If , for computing the -word that comes after  we need to invoke Algorithm~ several times. It is required to explain why the number of Lyndon words we concatenate is proportional to the suffix we seek for. More precisely, we need to show that the total number of invocations of Algorithm~ consumes  time. This is settled by the next lemma, which  claims that there are no two consecutive words, ,  both of length smaller than :



For , if , then . 





We can now present, in Algorithm~, a  algorithm based on a reduction to the  problem.



Consider the { loop in algorithm~ and use Lemma~ to conclude that after  loop iterations, which consume  time,  increases by at least  symbols. It follows that the loop halts in  steps and hence, we get the following:



If  can be computed in
  time, then Algorithm~ forms a  for the prefer-min -DB sequence with  time complexity.
 




In this section we construct an efficient -algorithm. This is done in two steps. First, we define the notion of a  of an -word, , and show how a cover for  can be transformed into  efficiently. Then, we show how to find a cover for an -word, , in linear time.








The  problem, applied on an -word, , is to extend  into a suffix of . 
For solving this problem, we introduce a similar notion.




For an -word, ,  if the following hold: 

        
In addition, we say that  is covered by , if  for some word . 


 Also here, we leave for the reader to verify that  is well-defined. We focus on -words different from  from technical reasons, as it allows us to provide a simpler presentation of our results. Otherwise, many parts in our analysis should be rephrased, and some proofs should be rewritten, to include more details. However, it is simple to show that the  algorithm we provide at the end of this section, works for every -word.

The two notions,  and  are closely related.
The difference between these notions can be bridged by observing that if  is a cover for , then  is a subword of . To clear this issue, we deal with the  relationships between two consecutive Lyndon words in the following Lemma.



For all  such that , if , then , for some -word, .




It follows that for  we have that  is a prefix of prefer-min. This trivially holds when , and if , the  previous lemma  ensures that there exists some  such that . Now we turn to deal with the relationships between  and .



Assume that  for an -word, . 

    

    
        When , we have  where  is the -prefix of . 

    






 Using the above, Algorithm~ transforms  into  in linear time.




We conclude this subsection with the next corollary. Its first item follows by Lemma~, and its second item follows from the code; for the input , line 1 assigns  to variable , and the condition in line 2 does not hold. Then,  is assigned with , and the algorithm returns , which is . 




Let  be an -word.

     returns  on the input .
    
     returns  on the input .













In this section we show how to compute , efficiently. Assume that an -word, , is covered by . Thus,  is a subword of . To compute  (in Algorithm~), in some cases, we compute  and use it to find , and in other cases we compute directly the suffix of  that follows . The way this goal is achieved relies on the analysis we provide here, which we divide into two parts. First, we show how to construct  from , by concatenating certain words to . Then, we present a structural characterization of  which will serve us to compute  . 




Assume that an -word, , is covered by . Hence,  is a subword of , but not of .
Clearly, Lemma~ implies that   is a prefix of , but what is the difference between these two sequences? The first goal of our analysis is to show how to construct  from , by concatenating a suffix to .




Let . We define a sequence of words:  and a sequence of indices:  by induction, where  indicates the amount of characters left to calculate in step :
Write  and assume that  were defined, together with  .

        


As an illustration of this definition, we give the following example:

 Let ,  and . Then,

        
        
        

Also, the reader may check that , as  Corollary~ states.  



We show now how the words  form as building blocks for constructing  from . We divide the analysis into three Lemmas, to deal with the different cases.



Take , and consider the words , as defined in Definition~. If , then: 









Take  , and consider the words , as defined in Definition~. If  and , then: 










Take , and consider the words , as defined in Definition~. If  and , then:








Now we can show how to construct  from .



Take , and consider the words , as defined in Definition~. Then, .




By the former lemma and by Lemmas~,~ and~, we also conclude:



Take , and consider the words , as defined in Definition~. 

    
        
        






















We are ready to present our analysis concerning the structure of an -word, , in order to extract information that we use to compute . First, we identify a distinguished simple case, and define:



An -word, , is said to be an expanded Lyndon word, if  for some .


If  is an expanded Lyndon word, then .  
The reader may observe that procedures  and , both described in subsection~, can be used to decide if  is an expanded Lyndon word efficiently, and to extract  in linear time in those cases. 

But what shall we do in the general case? Namely, if  is a subword of , but not a suffix of this sequence? As a first step for answering this question we invoke Corollary~, which establishes relationships between  and the words defined in Definition~, as the next lemma elaborates.




Let  be an -word which is not an expended-Lyndon-word. If  is covered by , then , where  is a proper non-empty suffix of , and  is a proper prefix of .






From the proof of Lemma~, we also conclude:



Assume that an -word , is not an expanded Lyndon word, and  is covered by . Write  as in Lemma~. Hence,  where  is the -suffix of .


This corollary suggests a direction for computing . Namely, finding  and finding the -suffix of . For extracting this data, first, we check if the subword of :  is not empty (i.e. if ). In the case  where , it follows that  is a rotation of , and this fact is used by Algorithm~ to find  and the -suffix of . In the case where , we use Lemma~ and Corollary~ to find a Lyndon word, , such that . Then,  can be found by applying Algorithm~ on . It is also required to compute  in this case. To summary, we set three goals for our analysis:

        
        
    
    
    
    We start with the first goal. The next lemma provides a criterion equivalent to .



Assume that an -word, , is not an expanded Lyndon word, and  is covered by . Write  as in Lemma~. Hence,   if and only if .



At first glance, the previous lemma does not seem applicable since we aim to compute , but we have to know  to determine if . In fact, Lemma~ actually serves as an intermediate property, which is equivalent to another property that concerns the structure of , and can be computed efficiently.



An -word, , is said to be almost-Lyndon, if , and  is an expanded Lyndon word.  






Assume that an -word, , is not an expanded Lyndon word, and  is covered by . Write  as in Lemma~ (possibly,  and then ). Thus,  is almost-Lyndon if and only if .





So far, we identified a structural property of  which testifies if  or not. We turn now to achieve our second and third goals, which are finding a Lyndon word  such that , and computing , when . For these purposes, we use the classic result by Chen, Fox and Lyndon~. The authors of~ (see also~) proved that every non-empty word, , can be uniquely factorized into Lyndon words:  such that . We name this decomposition: the -factorization of . In the next lemma we show the connection between the .





Assume that an -word,   is not an expanded Lyndon word, and  is covered by . Write  as in Lemma~, and assume also that . Let  be the 

    
        
    





By the previous lemma, by  Lemma~ and by Corollary~, we conclude the following consequence, which achieves the two remaining goals.



Assume that an -word,  , is not an expanded Lyndon word, and that  is covered by . Write  as in Lemma~, assume that , and let  be the -factorization of . If  is the first word in this factorization, different from , then:

        








We are finally ready to present our linear time  algorithm. In addition to the algorithms described earlier, we use the following procedures, all can be computed in linear time:

    
     which runs in linear time. See~ for a presentation of this technique. Extensions and a detailed discussion can be found in~.
    
    
    
        
    , or Shiloach's Algorithm~, both runs in linear time. 
    
        
     for details and runtime analysis.
    
        
    Given an -word, , this procedure checks if it is almost-Lyndon. If , where  does not start with , we test if  is an expanded Lyndon word. This occurs if and only if  holds.






Algorithm~ computes  in  time.




By Propositions~ and~ we conclude:





We proposed the notion of a generalized-shift-rule for a De Bruijn sequence which, unlike a shift-rule, allows to construct the entire De Bruijn sequence efficiently. 
We noted that a generalized-shift-rule for an -DB sequence runs in time , and presented an  time generalized-shift-rule for the well-known prefer-min De Bruijn sequence. By imposing a trivial reduction, as explained in the preliminaries section, our results provide a generalized-shift-rule for the prefer-max De Bruijn sequence as well.








