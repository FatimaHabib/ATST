 Figure  shows the explosions that
indicate how much reward the agent receives. Figure
b shows a short { from such
a model, sampling each state based on the previous sampled state. The environment's
initial state  is drawn from a distribution
. The environment also emits a reward,
. For a sequence of actions  let
 be the probability of
reaching  by starting in  and taking the actions in the
sequence. For a state action distribution , let
. Let the -step {
. Let
, and . The results in
this paper also permit the reward model to be similarly misspecified. For large ,
 will closely approximate 
. . For the remainder of the paper we focus on the special
case of deterministic dynamics and blind rollout policies. Figure a shows the
discounted return of the policies generated by DAgger-MC and
H-DAgger-MC, averaged over 50 independent trials. Figure b shows the results. Figure
c shows the results. The resulting data is used to update the model. For the reward model it is
$_{rrgt} = _{hrwd} -
_{}$.